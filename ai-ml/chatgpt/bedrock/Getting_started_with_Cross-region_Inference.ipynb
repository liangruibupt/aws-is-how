{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Cross-region Inference in Amazon Bedrock\n",
    "\n",
    "Inference profiles via Amazon Bedrock allow you to perform cross-region inference without the need to setup anything in a fully-managed, secure and private manner. Inference profiles is a managed feature built on Amazon Bedrock, offering generative AI application builders a seamless solution for managing traffic bursts and ensuring optimal availability and performance. By leveraging this feature, builders no longer have to spend time and effort building complex resiliency structure. Instead, inference profiles intelligently routes traffic across multiple opted-in regions, automatically adapting to peak utilization surges. Any inferences carried out through inference profiles will dynamically utilize on-demand capacity available from any of the configured regions, maximizing availability and throughput.\n",
    "\n",
    "## Key Features & Benefits\n",
    "\n",
    "Some of the key features include:\n",
    "\n",
    "* Access to capacity in different regions allowing generative AI workloads to scale with demand.\n",
    "* Access to region-agnostic models to achieve higher throughput.\n",
    "* Become resilient to any traffic bursts.\n",
    "* Ability to select between the pre-defined region sets suiting application needs.\n",
    "* Compatible with existing APIs.\n",
    "* No additional cost.\n",
    "* Same model pricing as the source region.\n",
    "\n",
    "The end-user experience is not impacted as the model behind cross-region inference remains the same, and builders can focus on writing logic for a differentiated application. \n",
    "\n",
    "Bedrock is the easiest way to build and scale generative AI applications. Cross region inference further enhances the usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by installing the dependencies to ensure we have a recent version\n",
    "#!pip install --quiet --upgrade --force-reinstall boto3 botocore awscli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper methods \n",
    "\n",
    "1. To allow assume role capability if needed\n",
    "2. Leverage the profile set up for boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_boto_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "    service_name: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if not service_name:\n",
    "        if runtime:\n",
    "            service_name='bedrock-runtime'\n",
    "        else:\n",
    "            service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38.27\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd21f8774-472d-495d-b46a-ccc68cc3cbf1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Mon, 02 Jun 2025 09:12:11 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '56645',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'd21f8774-472d-495d-b46a-ccc68cc3cbf1'},\n",
       "  'RetryAttempts': 0},\n",
       " 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-tg1-large',\n",
       "   'modelId': 'amazon.titan-tg1-large',\n",
       "   'modelName': 'Titan Text Large',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1:0',\n",
       "   'modelId': 'amazon.titan-image-generator-v1:0',\n",
       "   'modelName': 'Titan Image Generator G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1',\n",
       "   'modelId': 'amazon.titan-image-generator-v1',\n",
       "   'modelName': 'Titan Image Generator G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v2:0',\n",
       "   'modelId': 'amazon.titan-image-generator-v2:0',\n",
       "   'modelName': 'Titan Image Generator G1 v2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED', 'ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-premier-v1:0:8k',\n",
       "   'modelId': 'amazon.nova-premier-v1:0:8k',\n",
       "   'modelName': 'Nova Premier',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-premier-v1:0:20k',\n",
       "   'modelId': 'amazon.nova-premier-v1:0:20k',\n",
       "   'modelName': 'Nova Premier',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-premier-v1:0:1000k',\n",
       "   'modelId': 'amazon.nova-premier-v1:0:1000k',\n",
       "   'modelName': 'Nova Premier',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-premier-v1:0:mm',\n",
       "   'modelId': 'amazon.nova-premier-v1:0:mm',\n",
       "   'modelName': 'Nova Premier',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-premier-v1:0',\n",
       "   'modelId': 'amazon.nova-premier-v1:0',\n",
       "   'modelName': 'Nova Premier',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-premier-v1:0',\n",
       "   'modelId': 'amazon.titan-text-premier-v1:0',\n",
       "   'modelName': 'Titan Text G1 - Premier',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-pro-v1:0:24k',\n",
       "   'modelId': 'amazon.nova-pro-v1:0:24k',\n",
       "   'modelName': 'Nova Pro',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-pro-v1:0:300k',\n",
       "   'modelId': 'amazon.nova-pro-v1:0:300k',\n",
       "   'modelName': 'Nova Pro',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING',\n",
       "    'DISTILLATION',\n",
       "    'PREFERENCE_FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-pro-v1:0',\n",
       "   'modelId': 'amazon.nova-pro-v1:0',\n",
       "   'modelName': 'Nova Pro',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND', 'INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-lite-v1:0:24k',\n",
       "   'modelId': 'amazon.nova-lite-v1:0:24k',\n",
       "   'modelName': 'Nova Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-lite-v1:0:300k',\n",
       "   'modelId': 'amazon.nova-lite-v1:0:300k',\n",
       "   'modelName': 'Nova Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING',\n",
       "    'DISTILLATION',\n",
       "    'PREFERENCE_FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-lite-v1:0',\n",
       "   'modelId': 'amazon.nova-lite-v1:0',\n",
       "   'modelName': 'Nova Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE', 'VIDEO'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND', 'INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-canvas-v1:0',\n",
       "   'modelId': 'amazon.nova-canvas-v1:0',\n",
       "   'modelName': 'Nova Canvas',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND', 'PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-reel-v1:0',\n",
       "   'modelId': 'amazon.nova-reel-v1:0',\n",
       "   'modelName': 'Nova Reel',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['VIDEO'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-reel-v1:1',\n",
       "   'modelId': 'amazon.nova-reel-v1:1',\n",
       "   'modelName': 'Nova Reel',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['VIDEO'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-micro-v1:0:24k',\n",
       "   'modelId': 'amazon.nova-micro-v1:0:24k',\n",
       "   'modelName': 'Nova Micro',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-micro-v1:0:128k',\n",
       "   'modelId': 'amazon.nova-micro-v1:0:128k',\n",
       "   'modelName': 'Nova Micro',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING',\n",
       "    'DISTILLATION',\n",
       "    'PREFERENCE_FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-micro-v1:0',\n",
       "   'modelId': 'amazon.nova-micro-v1:0',\n",
       "   'modelName': 'Nova Micro',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND', 'INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-sonic-v1:0',\n",
       "   'modelId': 'amazon.nova-sonic-v1:0',\n",
       "   'modelName': 'Nova Sonic',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['SPEECH'],\n",
       "   'outputModalities': ['SPEECH', 'TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-g1-text-02',\n",
       "   'modelId': 'amazon.titan-embed-g1-text-02',\n",
       "   'modelName': 'Titan Text Embeddings v2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelId': 'amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1',\n",
       "   'modelId': 'amazon.titan-text-lite-v1',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1:0:8k',\n",
       "   'modelId': 'amazon.titan-text-express-v1:0:8k',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1',\n",
       "   'modelId': 'amazon.titan-text-express-v1',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1',\n",
       "   'modelId': 'amazon.titan-embed-text-v1',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0:8k',\n",
       "   'modelName': 'Titan Text Embeddings V2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0',\n",
       "   'modelName': 'Titan Text Embeddings V2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1:0',\n",
       "   'modelId': 'amazon.titan-embed-image-v1:0',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1',\n",
       "   'modelId': 'amazon.titan-embed-image-v1',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1:0',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1:0',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.jamba-instruct-v1:0',\n",
       "   'modelId': 'ai21.jamba-instruct-v1:0',\n",
       "   'modelName': 'Jamba-Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.jamba-1-5-large-v1:0',\n",
       "   'modelId': 'ai21.jamba-1-5-large-v1:0',\n",
       "   'modelName': 'Jamba 1.5 Large',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.jamba-1-5-mini-v1:0',\n",
       "   'modelId': 'ai21.jamba-1-5-mini-v1:0',\n",
       "   'modelName': 'Jamba 1.5 Mini',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1:2:100k',\n",
       "   'modelId': 'anthropic.claude-instant-v1:2:100k',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1',\n",
       "   'modelId': 'anthropic.claude-instant-v1',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:18k',\n",
       "   'modelId': 'anthropic.claude-v2:0:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:100k',\n",
       "   'modelId': 'anthropic.claude-v2:0:100k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:18k',\n",
       "   'modelId': 'anthropic.claude-v2:1:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:200k',\n",
       "   'modelId': 'anthropic.claude-v2:1:200k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1',\n",
       "   'modelId': 'anthropic.claude-v2:1',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2',\n",
       "   'modelId': 'anthropic.claude-v2',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-opus-20240229-v1:0:12k',\n",
       "   'modelId': 'anthropic.claude-3-opus-20240229-v1:0:12k',\n",
       "   'modelName': 'Claude 3 Opus',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-opus-20240229-v1:0:28k',\n",
       "   'modelId': 'anthropic.claude-3-opus-20240229-v1:0:28k',\n",
       "   'modelName': 'Claude 3 Opus',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-opus-20240229-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-opus-20240229-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Opus',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-opus-20240229-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-opus-20240229-v1:0',\n",
       "   'modelName': 'Claude 3 Opus',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "   'modelName': 'Claude 3.5 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND', 'INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
       "   'modelName': 'Claude 3.5 Sonnet v2',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
       "   'modelName': 'Claude 3.7 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-haiku-20241022-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-5-haiku-20241022-v1:0',\n",
       "   'modelName': 'Claude 3.5 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-opus-4-20250514-v1:0',\n",
       "   'modelId': 'anthropic.claude-opus-4-20250514-v1:0',\n",
       "   'modelName': 'Claude Opus 4',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-sonnet-4-20250514-v1:0',\n",
       "   'modelId': 'anthropic.claude-sonnet-4-20250514-v1:0',\n",
       "   'modelName': 'Claude Sonnet 4',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14:7:4k',\n",
       "   'modelId': 'cohere.command-text-v14:7:4k',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14',\n",
       "   'modelId': 'cohere.command-text-v14',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-r-v1:0',\n",
       "   'modelId': 'cohere.command-r-v1:0',\n",
       "   'modelName': 'Command R',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-r-plus-v1:0',\n",
       "   'modelId': 'cohere.command-r-plus-v1:0',\n",
       "   'modelName': 'Command R+',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14:7:4k',\n",
       "   'modelId': 'cohere.command-light-text-v14:7:4k',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14',\n",
       "   'modelId': 'cohere.command-light-text-v14',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3:0:512',\n",
       "   'modelId': 'cohere.embed-english-v3:0:512',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3',\n",
       "   'modelId': 'cohere.embed-english-v3',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3:0:512',\n",
       "   'modelId': 'cohere.embed-multilingual-v3:0:512',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3',\n",
       "   'modelId': 'cohere.embed-multilingual-v3',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/deepseek.r1-v1:0',\n",
       "   'modelId': 'deepseek.r1-v1:0',\n",
       "   'modelName': 'DeepSeek-R1',\n",
       "   'providerName': 'DeepSeek',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-8b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-8b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 8B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-70b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-70b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 70B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-1-8b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-1-8b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3.1 8B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-1-70b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-1-70b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3.1 70B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-2-11b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-2-11b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3.2 11B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-2-90b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-2-90b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3.2 90B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-2-1b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-2-1b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3.2 1B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-2-3b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-2-3b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3.2 3B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-3-70b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-3-70b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3.3 70B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama4-scout-17b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama4-scout-17b-instruct-v1:0',\n",
       "   'modelName': 'Llama 4 Scout 17B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama4-maverick-17b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama4-maverick-17b-instruct-v1:0',\n",
       "   'modelName': 'Llama 4 Maverick 17B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-7b-instruct-v0:2',\n",
       "   'modelId': 'mistral.mistral-7b-instruct-v0:2',\n",
       "   'modelName': 'Mistral 7B Instruct',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mixtral-8x7b-instruct-v0:1',\n",
       "   'modelId': 'mistral.mixtral-8x7b-instruct-v0:1',\n",
       "   'modelName': 'Mixtral 8x7B Instruct',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-large-2402-v1:0',\n",
       "   'modelId': 'mistral.mistral-large-2402-v1:0',\n",
       "   'modelName': 'Mistral Large (24.02)',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-small-2402-v1:0',\n",
       "   'modelId': 'mistral.mistral-small-2402-v1:0',\n",
       "   'modelName': 'Mistral Small (24.02)',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.pixtral-large-2502-v1:0',\n",
       "   'modelId': 'mistral.pixtral-large-2502-v1:0',\n",
       "   'modelName': 'Pixtral Large (25.02)',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "print(boto3.__version__)\n",
    "\n",
    "#os.environ[\"AWS_PROFILE\"] = '<replace with your profile>'\n",
    "if 'AWS_PROFILE' in os.environ:\n",
    "    del os.environ['AWS_PROFILE']\n",
    "boto3_client =  get_boto_client(region='us-east-1', runtime=False) # switch to the region of your choice\n",
    "boto3_client.list_foundation_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " IMPORTANT\n",
    "\n",
    "*Note:* <span style=\"color:red\">This notebook sample uses `us-east-1` region and the inference profiles available there as example.\n",
    "Change the `region_name` and inference profile ids in the cells below depending on which region you are in and which inference profiles are available.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "sts(https://sts.amazonaws.com)\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n",
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n",
      "710299592439 <botocore.client.Bedrock object at 0x7fc4559c3320> <botocore.client.Bedrock object at 0x7fc4559c3320>\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have AWS credentials or AWS profile setup before running this cell\n",
    "\n",
    "#boto3_client =  get_bedrock_client(region='us-east-1', runtime=False)\n",
    "\n",
    "region_name = 'us-east-1' # switch to the region of your choice\n",
    "account_id = get_boto_client(service_name='sts', region=region_name).get_caller_identity().get('Account')\n",
    "bedrock_client = get_boto_client(service_name='bedrock', region=region_name)\n",
    "bedrock_runtime = get_boto_client(service_name='bedrock-runtime', region=region_name)\n",
    "\n",
    "print(account_id, bedrock_client, bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Inference Profiles\n",
    "\n",
    "The cross-region inference feature comes with two additional APIs in the Bedrock client SDK:\n",
    "\n",
    "* `list_inference_profiles()` -> This API tells you all the models that are configured behind Inference Profiles for you.\n",
    "* `get_inference_profile(inferenceProfileIdentifier)` -> This API give you specific details of a certain Inference Profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'inferenceProfileName': 'US Anthropic Claude 3 Sonnet',\n",
       "  'description': 'Routes requests to Anthropic Claude 3 Sonnet in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0'}],\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Anthropic Claude 3 Opus',\n",
       "  'description': 'Routes requests to Anthropic Cluade 3 Opus in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-3-opus-20240229-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-opus-20240229-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-opus-20240229-v1:0'}],\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-opus-20240229-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Anthropic Claude 3 Haiku',\n",
       "  'description': 'Routes requests to Anthropic Claude 3 Haiku in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-3-haiku-20240307-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0'}],\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-haiku-20240307-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Meta Llama 3.2 11B Instruct',\n",
       "  'description': 'Routes requests to Meta Llama 3.2 11B Instruct in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 9, 25, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 9, 25, 0, 0, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.meta.llama3-2-11b-instruct-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-2-11b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-11b-instruct-v1:0'}],\n",
       "  'inferenceProfileId': 'us.meta.llama3-2-11b-instruct-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Meta Llama 3.2 3B Instruct',\n",
       "  'description': 'Routes requests to Meta Llama 3.2 3B Instruct in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 9, 25, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 9, 25, 0, 0, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.meta.llama3-2-3b-instruct-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-2-3b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-3b-instruct-v1:0'}],\n",
       "  'inferenceProfileId': 'us.meta.llama3-2-3b-instruct-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Meta Llama 3.2 90B Instruct',\n",
       "  'description': 'Routes requests to Meta Llama 3.2 90B Instruct in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 9, 25, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 9, 25, 0, 0, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.meta.llama3-2-90b-instruct-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-2-90b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-90b-instruct-v1:0'}],\n",
       "  'inferenceProfileId': 'us.meta.llama3-2-90b-instruct-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Meta Llama 3.2 1B Instruct',\n",
       "  'description': 'Routes requests to Meta Llama 3.2 1B Instruct in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 9, 25, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 9, 25, 0, 0, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.meta.llama3-2-1b-instruct-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-2-1b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-1b-instruct-v1:0'}],\n",
       "  'inferenceProfileId': 'us.meta.llama3-2-1b-instruct-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Anthropic Claude 3.5 Sonnet',\n",
       "  'description': 'Routes requests to Anthropic Claude 3.5 Sonnet in us-east-1 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 9, 26, 14, 9, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'}],\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Anthropic Claude 3.5 Haiku',\n",
       "  'description': 'Routes requests to US Anthropic Claude 3.5 Haiku in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 11, 4, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 11, 4, 0, 0, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-haiku-20241022-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/anthropic.claude-3-5-haiku-20241022-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-haiku-20241022-v1:0'}],\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Meta Llama 3.1 8B Instruct',\n",
       "  'description': 'Routes requests to Meta Llama 3.1 8B Instruct in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 11, 1, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 11, 4, 18, 4, 3, 991725, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.meta.llama3-1-8b-instruct-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-1-8b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/meta.llama3-1-8b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-8b-instruct-v1:0'}],\n",
       "  'inferenceProfileId': 'us.meta.llama3-1-8b-instruct-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Meta Llama 3.1 70B Instruct',\n",
       "  'description': 'Routes requests to Meta Llama 3.1 70B Instruct in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 11, 1, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 11, 4, 18, 4, 24, 303028, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.meta.llama3-1-70b-instruct-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-1-70b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/meta.llama3-1-70b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-70b-instruct-v1:0'}],\n",
       "  'inferenceProfileId': 'us.meta.llama3-1-70b-instruct-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Nova Lite',\n",
       "  'description': 'Routes requests to Nova Lite in us-east-1, us-west-2 and us-east-2.',\n",
       "  'createdAt': datetime.datetime(2024, 11, 29, 13, 23, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 11, 29, 13, 23, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.amazon.nova-lite-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-lite-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.nova-lite-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/amazon.nova-lite-v1:0'}],\n",
       "  'inferenceProfileId': 'us.amazon.nova-lite-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Nova Pro',\n",
       "  'description': 'Routes requests to Nova Pro in us-east-1, us-west-2 and us-east-2.',\n",
       "  'createdAt': datetime.datetime(2024, 11, 29, 13, 23, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 11, 29, 13, 23, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.amazon.nova-pro-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-pro-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.nova-pro-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/amazon.nova-pro-v1:0'}],\n",
       "  'inferenceProfileId': 'us.amazon.nova-pro-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Nova Micro',\n",
       "  'description': 'Routes requests to Nova Micro in us-east-1, us-west-2 and us-east-2.',\n",
       "  'createdAt': datetime.datetime(2024, 11, 29, 13, 23, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 11, 29, 13, 23, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.amazon.nova-micro-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-micro-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.nova-micro-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/amazon.nova-micro-v1:0'}],\n",
       "  'inferenceProfileId': 'us.amazon.nova-micro-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Meta Llama 3.3 70B Instruct',\n",
       "  'description': 'Routes requests to Meta Llama 3.3 70B Instruct in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2024, 12, 19, 1, 30, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 12, 19, 22, 6, 30, 753678, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.meta.llama3-3-70b-instruct-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-3-70b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/meta.llama3-3-70b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-3-70b-instruct-v1:0'}],\n",
       "  'inferenceProfileId': 'us.meta.llama3-3-70b-instruct-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Anthropic Claude 3.5 Sonnet v2',\n",
       "  'description': 'Routes requests to Anthropic Claude 3.5 Sonnet v2 in us-west-2, us-east-1 and us-east-2.',\n",
       "  'createdAt': datetime.datetime(2024, 10, 22, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2025, 2, 12, 22, 55, 19, 264088, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-sonnet-20241022-v2:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20241022-v2:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/anthropic.claude-3-5-sonnet-20241022-v2:0'}],\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US DeepSeek-R1',\n",
       "  'description': 'Routes requests to DeepSeek-R1 in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2025, 3, 7, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2025, 3, 10, 22, 8, 43, 461730, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.deepseek.r1-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/deepseek.r1-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/deepseek.r1-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/deepseek.r1-v1:0'}],\n",
       "  'inferenceProfileId': 'us.deepseek.r1-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Mistral Pixtral Large 25.02',\n",
       "  'description': 'Routes requests to Mistral Pixtral Large 25.02 in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2025, 4, 4, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2025, 4, 7, 20, 46, 41, 272192, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.mistral.pixtral-large-2502-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.pixtral-large-2502-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/mistral.pixtral-large-2502-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/mistral.pixtral-large-2502-v1:0'}],\n",
       "  'inferenceProfileId': 'us.mistral.pixtral-large-2502-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Llama 4 Scout 17B Instruct',\n",
       "  'description': 'Routes requests to Llama 4 Scout 17B Instruct in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2025, 4, 25, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2025, 4, 29, 1, 0, 15, 949959, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.meta.llama4-scout-17b-instruct-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama4-scout-17b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/meta.llama4-scout-17b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama4-scout-17b-instruct-v1:0'}],\n",
       "  'inferenceProfileId': 'us.meta.llama4-scout-17b-instruct-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Llama 4 Maverick 17B Instruct',\n",
       "  'description': 'Routes requests to Llama 4 Maverick 17B Instruct in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2025, 4, 25, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2025, 4, 29, 1, 1, 46, 907877, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.meta.llama4-maverick-17b-instruct-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama4-maverick-17b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/meta.llama4-maverick-17b-instruct-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama4-maverick-17b-instruct-v1:0'}],\n",
       "  'inferenceProfileId': 'us.meta.llama4-maverick-17b-instruct-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Nova Premier',\n",
       "  'description': 'Routes requests to Nova Premier in us-east-1, us-west-2 and us-east-2.',\n",
       "  'createdAt': datetime.datetime(2025, 3, 23, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2025, 4, 30, 22, 29, 14, 461563, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.amazon.nova-premier-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-premier-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.nova-premier-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/amazon.nova-premier-v1:0'}],\n",
       "  'inferenceProfileId': 'us.amazon.nova-premier-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Anthropic Claude 3.7 Sonnet',\n",
       "  'description': 'Routes requests to Anthropic Claude 3.7 Sonnet in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2025, 2, 21, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2025, 5, 22, 1, 46, 51, 936252, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-7-sonnet-20250219-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/anthropic.claude-3-7-sonnet-20250219-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-7-sonnet-20250219-v1:0'}],\n",
       "  'inferenceProfileId': 'us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Claude Sonnet 4',\n",
       "  'description': 'Routes requests to Claude Sonnet 4 in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2025, 5, 20, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2025, 5, 22, 21, 44, 0, 14062, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-sonnet-4-20250514-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/anthropic.claude-sonnet-4-20250514-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-sonnet-4-20250514-v1:0'}],\n",
       "  'inferenceProfileId': 'us.anthropic.claude-sonnet-4-20250514-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'},\n",
       " {'inferenceProfileName': 'US Claude Opus 4',\n",
       "  'description': 'Routes requests to Claude Opus 4 in us-east-1, us-east-2 and us-west-2.',\n",
       "  'createdAt': datetime.datetime(2025, 5, 12, 0, 0, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2025, 5, 22, 21, 48, 45, 464845, tzinfo=tzlocal()),\n",
       "  'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-opus-4-20250514-v1:0',\n",
       "  'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-opus-4-20250514-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-east-2::foundation-model/anthropic.claude-opus-4-20250514-v1:0'},\n",
       "   {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-opus-4-20250514-v1:0'}],\n",
       "  'inferenceProfileId': 'us.anthropic.claude-opus-4-20250514-v1:0',\n",
       "  'status': 'ACTIVE',\n",
       "  'type': 'SYSTEM_DEFINED'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_client.list_inference_profiles()['inferenceProfileSummaries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that cross-region inference offers two forms:\n",
    "\n",
    "**Foundation model in source region**\n",
    "\n",
    "In this mode a Inference Profile is only configured for a model which exists in the source region. For such model(s) a failover mechanism would exist allowing requests to be re-routed to fulfilment regions configured in Inference Profile. By default the request will go to source region resources and only if the region is busy or you hit your quota limit, the request is routed to another region. In order to determine, which region the request should go to Amazon Bedrock intelligently checks in real-time which region has redundant capacity available. The on-demand principle still applies if none of the region has capacity to handle the request, then it will be throttled.\n",
    "\n",
    "In this mode, If the source region doesn't have a certain model available, you will not be able to access it in a fulfilment region via cross-region inference feature.\n",
    "\n",
    "**Available via Inference Profiles**\n",
    "\n",
    "Select list of models are made available via cross-region inference, where Amazon Bedrock abstracts away the regional details and manages the hosting and inference routing automatically. Such foundation models then exist across the pre-defined region sets and the builder can build applications agnostic of setting up a region. This allows reliable throughput, access to leading foundation models as well as scalability in terms of throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'dbc02497-6e5e-4901-817a-806ddf895612',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Mon, 02 Jun 2025 09:12:43 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '667',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'dbc02497-6e5e-4901-817a-806ddf895612'},\n",
       "  'RetryAttempts': 0},\n",
       " 'inferenceProfileName': 'US Anthropic Claude 3.5 Sonnet',\n",
       " 'description': 'Routes requests to Anthropic Claude 3.5 Sonnet in us-east-1 and us-west-2.',\n",
       " 'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzlocal()),\n",
       " 'updatedAt': datetime.datetime(2024, 9, 26, 14, 9, tzinfo=tzlocal()),\n",
       " 'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'}],\n",
       " 'inferenceProfileId': 'us.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'status': 'ACTIVE',\n",
       " 'type': 'SYSTEM_DEFINED'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_client.get_inference_profile(\n",
    "    inferenceProfileIdentifier='us.anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `get_inference_profile` API you can observe the `status` if a Inference Profile is `ACTIVE` or not and also which regions are configured for inference routing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Inference Profiles\n",
    "\n",
    "An Inference Profile is used in the same way as a foundation model using the `modelId` or `arn` of the model. Inference profile also comes with it's own id and arn, where the difference is in the prefix. For the inference profile you can expect a regional prefix such as `us.` or `eu.` behind the model id for it to be recognized as an inference profile. Also in the `arn`, you can find the difference from `:<region>::foundation-model/<model-id>` to `:<region>::inference-profile/<region-set-prefix>.<model-id>`\n",
    "\n",
    "You can use both the `arn` and the `modelId` with `Converse` API whereas only the `modelId` with `InvokeModel` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converse API\n",
    "\n",
    "Amazon Bedrock now supports a unified messaging API for seamless application building experience. Read about all the models supported via this API [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features).\n",
    "\n",
    "Let's send a request to both the foundation model as well as the Inference Profile to observe change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Foundation Model::Response time: 4 second(s)\n",
      "::model:id:anthropic.claude-3-haiku-20240307-v1:0::Response time: 4 second(s)\n",
      "::Foundation Model::Response output: [{'text': 'The choice between Amazon S3 or Amazon EFS for storing documents depends on your specific use case and requirements. Here are some factors to consider:\\n\\n1. **Storage Type**:\\n   - **Amazon S3**: S3 is an object storage service, which is ideal for storing and retrieving large files, such as documents, images, and other unstructured data.\\n   - **Amazon EFS**: EFS is a file storage service, which provides a shared file system that can be accessed by multiple EC2 instances simultaneously.\\n\\n2. **Access Pattern**:\\n   - **Amazon S3**: S3 is optimized for high-throughput, low-latency access to data, making it well-suited for use cases where you need to access files frequently or in parallel.\\n   - **Amazon EFS**: EFS is designed for file-based applications that require low-latency, high-throughput access to data, making it suitable for use cases where multiple EC2 instances need to access the same files concurrently.\\n\\n3. **Pricing**:\\n   - **Amazon S3**: S3 pricing is based on the amount of data stored, the number of requests, and the data transfer. S3 is generally more cost-effective for storing large amounts of infrequently accessed data.\\n   - **Amazon EFS**: EFS pricing is based on the amount of data stored and the amount of throughput used. EFS can be more cost-effective for applications that require frequent access to the same files.\\n\\n4. **Scalability and Durability**:\\n   - **Amazon S3**: S3 is highly scalable and durable, with built-in redundancy and automatic replication to multiple Availability Zones.\\n   - **Amazon EFS**: EFS is also highly scalable and durable, with the ability to automatically scale up or down based on your storage needs.\\n\\nBased on the information provided, if your application requires cost-effective storage of documents that are accessed infrequently, Amazon S3 would be the more cost-effective option. However, if your application requires frequent, concurrent access to the same documents by multiple EC2 instances, then Amazon EFS might be the better choice.'}]\n",
      "::Inference Profile::Response time: 5 second(s)\n",
      "::model:id:us.anthropic.claude-3-haiku-20240307-v1:0::Response time: 5 second(s)\n",
      "::Inference Profile::Response output: [{'text': \"For cost-effective applications, Amazon S3 is generally a better choice for storing documents compared to Amazon EFS (Elastic File System).\\n\\nHere's why:\\n\\n1. Storage Costs:\\n   - S3 offers lower storage costs per gigabyte compared to EFS, especially for large volumes of data.\\n   - S3 has a pay-as-you-go pricing model, where you only pay for the storage you use.\\n   - EFS has a more complex pricing structure, with charges for storage, throughput, and data transfers.\\n\\n2. Scalability:\\n   - S3 is designed to scale automatically to handle virtually unlimited amounts of data without the need to provision or manage storage capacity.\\n   - EFS has a scalable file system, but you need to provision the storage capacity and performance based on your application's needs.\\n\\n3. Use Cases:\\n   - S3 is well-suited for storing unstructured data, such as documents, images, and files, where you don't need a traditional file system.\\n   - EFS is more appropriate for applications that require a fully-featured file system, such as shared file storage for multiple EC2 instances or on-premises systems.\\n\\n4. Availability and Durability:\\n   - S3 provides high availability and durability, with built-in redundancy and replication across multiple Availability Zones.\\n   - EFS also offers high availability and durability, but it may have higher maintenance overhead compared to S3.\\n\\nFor cost-effective applications that primarily involve storing and retrieving documents, Amazon S3 is generally the more suitable and cost-effective choice. S3 offers lower storage costs, simpler pricing, and better scalability for unstructured data storage needs.\\n\\nHowever, if your application requires a fully-featured file system with shared access from multiple instances, then Amazon EFS may be the better option, even though it may be more expensive.\"}]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "system_prompt = \"You are an expert on AWS services and always provide correct and concise answers.\"\n",
    "input_message = \"Should I be storing documents in Amazon S3 or EFS for cost effective applications?\"\n",
    "modelId = ('Foundation Model', 'anthropic.claude-3-haiku-20240307-v1:0')\n",
    "inferenceProfileId = ('Inference Profile', 'us.anthropic.claude-3-haiku-20240307-v1:0')\n",
    "\n",
    "for inference_type, id in [modelId, inferenceProfileId]:\n",
    "    start = time()\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=id,\n",
    "        system=[{\"text\": system_prompt}],\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": input_message}]\n",
    "        }]\n",
    "    )\n",
    "    end = time()\n",
    "    print(f\"::{inference_type}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::model:id:{id}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::{inference_type}::Response output: {response['output']['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that using the same API and only the change in model ID you can expect similar behavior.\n",
    "\n",
    "It is also possible to use a full ARN in place of the model ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:bedrock:us-east-1:710299592439:inference-profile/us.anthropic.claude-3-sonnet-20240229-v1:0\n",
      "The choice between Amazon S3 (Simple Storage Service) and Amazon EFS (Elastic File System) for storing documents depends on your specific use case and requirements. Here's a general comparison to help you decide:\n",
      "\n",
      "**Amazon S3**:\n",
      "- Object storage service designed for highly scalable and durable storage of flat files (documents, images, videos, etc.)\n",
      "- Provides low-cost storage with varying tiers (e.g., Standard, Intelligent-Tiering, Glacier) based on access patterns\n",
      "- Data is distributed across multiple facilities and devices for redundancy\n",
      "- Ideal for storing static content, backups, archives, and data lakes\n",
      "- Accessible over the internet or within AWS resources\n",
      "- Highly durable (99.999999999% durability) and available\n",
      "- Scaling is automatic and virtually unlimited\n",
      "- Pay-per-use pricing model based on storage usage, requests, and data transfer\n",
      "\n",
      "**Amazon EFS**:\n",
      "- Managed file system service that provides easy-to-use, scalable file storage\n",
      "- Designed for use cases that require shared access to the same data from multiple EC2 instances or on-premises servers\n",
      "- Supports the Network File System (NFS) protocol\n",
      "- Suitable for applications that require shared file access, such as content management systems, web servers, data analytics, and media processing workflows\n",
      "- Provides high levels of throughput and IOPS (Input/Output Operations Per Second)\n",
      "- Automatically scales capacity and performance as file system grows\n",
      "- Pay-per-use pricing model based on storage usage and provisioned throughput\n",
      "\n",
      "For cost-effective applications that don't require shared file access and can work with object storage, Amazon S3 is generally the more cost-effective option, especially for storing large amounts of data. S3 offers varying storage classes to optimize costs based on access patterns.\n",
      "\n",
      "However, if your application requires shared file access from multiple compute resources, or if you need the semantics of a traditional file system (e.g., locking, strong consistency), Amazon EFS would be a better choice, albeit at a higher cost compared to S3.\n",
      "\n",
      "Ultimately, the decision should be based on your specific application requirements, access patterns, and cost considerations. You can also consider using both services in combination if your workload demands it.\n"
     ]
    }
   ],
   "source": [
    "inference_profile_id = \"us.anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "inference_profile_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{inference_profile_id}\"\n",
    "print(inference_profile_arn)\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inference_profile_arn,\n",
    "    system=[{\"text\": system_prompt}],\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_message}]\n",
    "    }]\n",
    ")\n",
    "print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InvokeModel API\n",
    "\n",
    "Most of the generative AI applications in production are already built on top of `InvokeModel` API, even the third-party tools also use this API for using the models. The cross-region inference feature is also compatible with this API. Where the Converse API only supports select models, all models available in Amazon Bedrock can leverage InvokeModel API.\n",
    "\n",
    "Let's send a request via both the foundation model id as well as the inference profile id to observe change via this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Foundation Model::Response time: 19 second(s)\n",
      "::Foundation Model::Response output: The choice between Amazon S3 (Simple Storage Service) and Amazon EFS (Elastic File System) for storing documents depends on your specific use case and requirements. Here's a general comparison to help you decide:\n",
      "\n",
      "**Amazon S3**:\n",
      "- S3 is an object storage service designed for storing and retrieving any amount of data from anywhere on the internet.\n",
      "- It is highly scalable, durable, and cost-effective for storing large amounts of data.\n",
      "- S3 is ideal for storing static files, backups, archives, media files, and other types of unstructured data.\n",
      "- Data is stored as objects (files) within buckets, and you can set different storage classes based on access patterns to optimize costs.\n",
      "- S3 provides high availability and durability through data replication across multiple Availability Zones.\n",
      "- Access to S3 is over the internet, which may not be suitable for low-latency or high-throughput workloads.\n",
      "\n",
      "**Amazon EFS**:\n",
      "- EFS is a fully-managed file storage service that provides a scalable file system for use with AWS Cloud services and on-premises resources.\n",
      "- It is designed for workloads that require shared access to the same data from multiple compute resources, such as web servers, application servers, or data processing systems.\n",
      "- EFS provides low-latency access to data, making it suitable for applications that require high throughput and low latency.\n",
      "- Data is stored in a traditional file system hierarchy, which can be mounted on multiple EC2 instances or on-premises servers.\n",
      "- EFS is more expensive than S3 for storing large amounts of data, but it offers better performance for shared file access.\n",
      "- It provides built-in data redundancy and automatic failure recovery within an Availability Zone.\n",
      "\n",
      "For cost-effective applications that primarily involve storing and retrieving large amounts of unstructured data, such as documents, media files, or backups, Amazon S3 is generally the more cost-effective choice. S3 offers lower storage costs, especially for infrequently accessed data, and provides high durability and scalability.\n",
      "\n",
      "However, if your application requires low-latency shared access to files from multiple compute resources, or if you need a traditional file system interface, Amazon EFS may be a better choice, despite being more expensive than S3 for storage alone.\n",
      "\n",
      "Ultimately, the decision should be based on your specific requirements for performance, access patterns, data sharing needs, and cost considerations.\n",
      "::Inference Profile::Response time: 24 second(s)\n",
      "::Inference Profile::Response output: The choice between Amazon S3 (Simple Storage Service) and Amazon EFS (Elastic File System) for storing documents depends on your specific use case and requirements. Here's a general comparison to help you decide:\n",
      "\n",
      "**Amazon S3**:\n",
      "- S3 is an object storage service designed for storing and retrieving any amount of data from anywhere on the internet.\n",
      "- It is highly scalable, durable, and cost-effective for storing large amounts of data.\n",
      "- S3 is ideal for storing static files, backups, logs, media files, and other types of unstructured data.\n",
      "- Data is stored as objects (files) within buckets, and you can set different storage classes based on access patterns to optimize costs.\n",
      "- S3 provides high availability and durability through data replication across multiple Availability Zones.\n",
      "- Access to S3 is over the internet, which may not be suitable for low-latency or high-throughput workloads.\n",
      "\n",
      "**Amazon EFS**:\n",
      "- EFS is a fully-managed file storage service that provides a scalable file system for use with AWS Cloud services and on-premises resources.\n",
      "- It is designed for workloads that require shared access to the same data from multiple compute resources, such as web servers, application servers, or data processing systems.\n",
      "- EFS provides low-latency access to data, making it suitable for applications that require high throughput and low latency.\n",
      "- Data is stored in a traditional file system hierarchy, which can be mounted on multiple EC2 instances or on-premises servers.\n",
      "- EFS is more expensive than S3 for storing large amounts of data, but it offers better performance for shared file access.\n",
      "- It provides built-in data redundancy and automatic failure recovery within an Availability Zone.\n",
      "\n",
      "For cost-effective applications that primarily involve storing and retrieving large amounts of unstructured data, such as documents, media files, or backups, Amazon S3 is generally the more cost-effective choice. S3 offers lower storage costs, especially for infrequently accessed data, and provides high durability and scalability.\n",
      "\n",
      "However, if your application requires low-latency shared access to files from multiple compute resources, or if you need a traditional file system interface, Amazon EFS may be a better choice, despite being more expensive than S3 for storage alone.\n",
      "\n",
      "Ultimately, the decision should be based on your specific requirements for performance, access patterns, data sharing needs, and cost considerations.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"system\": system_prompt,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"{input_message}\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "modelId = ('Foundation Model', 'anthropic.claude-3-sonnet-20240229-v1:0')\n",
    "inferenceProfileId = ('Inference Profile', 'us.anthropic.claude-3-sonnet-20240229-v1:0')\n",
    "for inference_type, id in [modelId, inferenceProfileId]:\n",
    "    start = time()\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=id, accept=accept, contentType=contentType)\n",
    "    end = time()\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    print(f\"::{inference_type}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::{inference_type}::Response output: {response_body['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "\n",
    "Below you can find integration on how to use the new cross-region inference feature with one of the popular open-source frameworks [LangChain](https://python.langchain.com/v0.2/docs/integrations/platforms/aws/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain_aws langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangChain](https://python.langchain.com/v0.2/docs/introduction/) with the help of integrations implements [`langchain-aws`](https://github.com/langchain-ai/langchain-aws) which provides support  `Converse` API via [`ChatBedrockConverse`](https://python.langchain.com/v0.2/docs/integrations/chat/bedrock/#beta-bedrock-converse-api). This allows you to use the latest models such as Anthropic Claude 3 Sonnet via this implementation. Below you can see an example of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentence \"I love programming\" shaped into a poetic form:\n",
      "\n",
      "Code flows like poetry,\n",
      "Pixels dance with ecstasy.\n",
      "Logic's rhythm, my heart's key,\n",
      "Programming, my true reverie.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the ChatBedrock with Model id as the inference profile model id instead of the ARN\n",
    "\n",
    "**Option 1**\n",
    "\n",
    "1. We set the converse api flag to be true and that invokes the ChatBedrockConverse class which will invoke the model using the converse api class\n",
    "2. `Region` needs to be explictly passed in for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentence \"I love programming\" shaped into a poetic form:\n",
      "\n",
      "Code flows like poetry,\n",
      "Pixels dance with ecstasy.\n",
      "Logic's rhythm, my heart's key,\n",
      "Programming, my true reverie.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 200}\n",
    "llm = ChatBedrock(\n",
    "    model_id='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True,  \n",
    "    client=bedrock_runtime,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Model parameters\n",
    "\n",
    "Pass in the `Model inference parameters` at run time for model invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a poetic take on \"I love programming because...\":\n",
      "\n",
      "In lines of code, a world unfurls,\n",
      "Where logic reigns and creativity whirls.\n",
      "I love programming, for it's an art,\n",
      "A canvas to paint, where ideas start.\n",
      "\n",
      "With each keystroke, a new path is paved,\n",
      "Challenges conquered, solutions engraved.\n",
      "From algorithms dancing in intricate ways,\n",
      "To interfaces shining like a thousand rays.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Generate a potery from this cue\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming because......\"),\n",
    "]\n",
    "#model_parameter = {\"temperature\": 0.1, \"top_p\": .5, \"max_tokens\": 1000  } #\"max_tokens_to_sample\": 200}\n",
    "llm = ChatBedrock(\n",
    "    model_id='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    #model_kwargs=model_parameter,\n",
    "    beta_use_converse_api=True,\n",
    "    client=bedrock_runtime,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "# - these will take precedence even if we have similiar params when creating the class\n",
    "runtime_parameter = {\"temperature\": 0.9,  \"max_tokens\": 100 }\n",
    "print(llm.invoke(messages, **runtime_parameter).content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming with LangChain\n",
    "\n",
    "Demostrate the streaming capabilities with LangChain. We also demonstrate that run time parameters take precedence. Say if you have temperature when creating the ChatBedrock class and also during the invocation, it will use the invocation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a poetic take on \"I love programming because...\":\n",
      "\n",
      "Code dances on the screen, a symphony\n",
      "Of logic's brilliant hues, entrancing me.\n",
      "I love programming because it's an art,\n",
      "Where mind and machine become one, never to part.\n",
      "\n",
      "Each line, a brushstroke on a digital canvas,\n",
      "Crafting solutions, leaving no chance amiss.\n",
      "I love programming for the thrill it brings,\n",
      "Solving puzzles,"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Generate a potery from this cue\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming because......\"),\n",
    "]\n",
    "model_parameter = {\"temperature\": 0.1, \"top_p\": .5, \"max_tokens\": 1000  } #\"max_tokens_to_sample\": 200}\n",
    "llm = ChatBedrock(\n",
    "    model_id='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    model_kwargs=model_parameter,\n",
    "    beta_use_converse_api=True,\n",
    "    client=bedrock_runtime,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# These would take precedence \n",
    "runtime_parameter = {\"temperature\": 0.9,  \"max_tokens\": 100 }\n",
    "response = llm.stream(messages, **runtime_parameter)\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "for chunk in response:\n",
    "    if chunk and len(chunk.content) > 0:\n",
    "        sys.stdout.write(chunk.content[0].get('text',\"\"))\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2 where we have the model provider**\n",
    "\n",
    "This does not use the converse api behind the scenes and calls the invoke api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentence translated into poetic form:\n",
      "\n",
      "Coding's my passion, a love so true,\n",
      "Crafting lines of logic, a dance anew.\n",
      "Algorithms and syntax, a symphony to behold,\n",
      "Programming's embrace, a story untold.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5}\n",
    "llm = ChatBedrock(\n",
    "    provider=\"anthropic\",\n",
    "    model_id=\"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the invoke and the prescribed message format for Claude models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the user's sentence translated into poetic form:\n",
      "\n",
      "Coding's my passion, a symphony divine,\n",
      "Weaving lines of logic, a dance sublime.\n",
      "Algorithms and syntax, my heart's delight,\n",
      "Programming, my muse, ignites my mind's light.\n"
     ]
    }
   ],
   "source": [
    "messages = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"system\": 'You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.',\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"I love programming.\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5}\n",
    "llm = ChatBedrock(\n",
    "    provider=\"anthropic\",\n",
    "    model_id=\"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring, Logging, and Metrics\n",
    "\n",
    "Using cross-region inference might route your request to a different region, based on the selected inference profile.\n",
    "\n",
    "If a request gets re-routed, the [Bedrock model invocation log](https://docs.aws.amazon.com/bedrock/latest/userguide/model-invocation-logging.html) will mention the used region in the `inferenceRegion` element of the JSON log entry.\n",
    "\n",
    "The below code snippet will enable the model invocation logging to a CloudWatch log group and then create a CloudWatch metric filter to select and count all model invocations that get routed to a fulfilment region. You can adapt this code to build a metric for a specific target region and to monitor latency based on region.\n",
    "\n",
    "This code snippet creates a new IAM role with appropriate permissions to enable the model invocation log delivered into CloudWatch Logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the PutModelInvocationLoggingConfiguration operation: Failed to validate permissions for log group: bedrock-model-invocation-logging, with role: arn:aws:iam::710299592439:role/AmazonBedrockModelInvocationCWDeliveryRole. Verify the IAM role permissions are correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 66\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     61\u001b[0m logs_client\u001b[38;5;241m.\u001b[39mput_retention_policy(\n\u001b[1;32m     62\u001b[0m     logGroupName\u001b[38;5;241m=\u001b[39mlog_group_name,\n\u001b[1;32m     63\u001b[0m     retentionInDays\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m365\u001b[39m\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_model_invocation_logging_configuration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloggingConfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcloudWatchConfig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogGroupName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_group_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroleArn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marn:aws:iam::\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maccount_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:role/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrole_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimageDataDeliveryEnabled\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimageDataDeliveryEnabled\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddingDataDeliveryEnabled\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/botocore/client.py:595\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    592\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    593\u001b[0m     )\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/botocore/client.py:1058\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m     )\n\u001b[1;32m   1057\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the PutModelInvocationLoggingConfiguration operation: Failed to validate permissions for log group: bedrock-model-invocation-logging, with role: arn:aws:iam::710299592439:role/AmazonBedrockModelInvocationCWDeliveryRole. Verify the IAM role permissions are correct."
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# see https://docs.aws.amazon.com/bedrock/latest/userguide/model-invocation-logging.html\n",
    "\n",
    "log_group_name = 'bedrock-model-invocation-logging'\n",
    "role_name = 'AmazonBedrockModelInvocationCWDeliveryRole'\n",
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": account_id,\n",
    "                },\n",
    "                \"ArnLike\": {\n",
    "                    \"aws:SourceArn\": f\"arn:aws:bedrock:{region_name}:{account_id}:*\",\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "iam_client = boto3.client('iam', region_name=region_name)\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "    )\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    pass\n",
    "policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\",\n",
    "            ],\n",
    "            \"Resource\": f\"arn:aws:logs:{region_name}:{account_id}:log-group:{log_group_name}:log-stream:aws/bedrock/modelinvocations\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "iam_client.put_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyName=role_name,\n",
    "    PolicyDocument=json.dumps(policy_doc),\n",
    ")\n",
    "\n",
    "logs_client = boto3.client('logs', region_name=region_name)\n",
    "try:\n",
    "    logs_client.create_log_group(\n",
    "        logGroupName=log_group_name,\n",
    "    )\n",
    "except logs_client.exceptions.ResourceAlreadyExistsException:\n",
    "    pass\n",
    "logs_client.put_retention_policy(\n",
    "    logGroupName=log_group_name,\n",
    "    retentionInDays=365\n",
    ")\n",
    "\n",
    "response = bedrock_client.put_model_invocation_logging_configuration(\n",
    "    loggingConfig={\n",
    "        'cloudWatchConfig': {\n",
    "            'logGroupName': log_group_name,\n",
    "            'roleArn': f\"arn:aws:iam::{account_id}:role/{role_name}\",\n",
    "        },\n",
    "        'imageDataDeliveryEnabled': False,\n",
    "        'imageDataDeliveryEnabled': False,\n",
    "        'embeddingDataDeliveryEnabled': False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Model Invocation Log enabled and configured for CloudWatch Logs, you can now create a CloudWatch metric filter from the ingested JSON log entries by filtering for `inferenceRegion` to match / mis-match against your source or target region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_client.put_metric_filter(\n",
    "    logGroupName=log_group_name,\n",
    "    filterName='bedrock_cross_region_inference_rerouted',\n",
    "    filterPattern=f'{{ $.inferenceRegion != \"{region_name}\" }}',\n",
    "    metricTransformations=[\n",
    "        {\n",
    "            'metricNamespace': 'Custom',\n",
    "            'metricName': 'bedrock_cross_region_inference_rerouted',\n",
    "            'metricValue': '1',\n",
    "            'defaultValue': 0,\n",
    "            'unit': 'Count',\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Cross-region inference provides the ability to manage bursts and spiky traffic patterns across a variety of generative AI workloads and disparate request shapes.With this feature you can easily scale your workloads in production without the need of heavy-lifting, lengthy migrations and overhead of infrastructure management. Amazon Bedrock handles the routing securely, reliably and in a transparent manner while giving you the control you need.\n",
    "\n",
    "### Key considerations\n",
    "While building or migrating applications to use of cross-region inference, it is important to keep in mind the following:\n",
    "- **Latency** - If your application is latency sensitive, it is advised to properly test the use of cross-region inference prior to fully relying on it since the routing to different regions could lead to higher latency numbers thus impacting your application behavior.\n",
    "- **Compliance** - Cross-region inference comes with pre-defined region sets, if these sets contain a region where you can't operate or goes against your policy, then it is advised not to use Inference Profiles, instead utilize Foundation Models directly.\n",
    "- **Determinism** - If you need to have control over where your requests are (or will be) re-routed (other than source region), it is better to consider just rely on Foundation Model directly. Also the model exclusive to cross-region inference exclusive models should be opted for carefully.\n",
    "- **Variety** - Inference Profiles do not provide access to multiple different models from different regions, it either acts as a failover mechanism for models in source region or provides Inference Profile exclusive models where the construct of a region is abstracted away. If your business demands to span across variety of foundation models from multiple regions, it is wise to consider building your own architecture via VPC Peering/Transit Gateway or other architectural patterns.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "If you ran the **Monitoring, Logging, and Metrics** code, consider disabling the Model Invocation Log to avoid incuring associated cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
