{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Query Knowledge Base and Build RAG-powered Q&A Application with **Retrieve API**\n",
    "\n",
    "----\n",
    "\n",
    "This notebook provides sample code and step-by-step instructions for building a question-answering (Q&A) application using a **Retrieve API** of Amazon Bedrock Knowledge Bases.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In the previous notebook, we explored the `RetrieveAndGenerate` API from Amazon Bedrock Knowledge Bases — a fully managed RAG (Retrieval-Augmented Generation) solution. As the name suggests, this API not only retrieves the most relevant information from a knowledge base but also automatically generates a response to the user query in a single, fully managed API call.\n",
    "\n",
    "In this notebook, we will take a closer look at the `Retrieve` API, which provides greater flexibility for building custom RAG solutions. Unlike `RetrieveAndGenerate`, the `Retrieve` API only fetches relevant document chunks from a Knowledge Base based on the user query — leaving it up to the developer to decide how to leverage this retrieved information.\n",
    "\n",
    "To keep things simple and focused, in this notebook we will use the output of the `Retrieve` API to manually construct an augmented prompt. We will then send this prompt to a Bedrock's foundation model (FM) of our choice to generate a grounded response.\n",
    "\n",
    "![retrieveAPI](./images/retrieve_api.png)\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "In order to run this notebook, you should have successfully completed the first notebook lab:\n",
    "- [1_create-kb-and-ingest-documents.ipynb](./1\\_create-kb-and-ingest-documents.ipynb).\n",
    "\n",
    "Also, please make sure that you have enabled the following model access in _Amazon Bedrock Console_:\n",
    "\n",
    "- `Amazon Nova Micro`\n",
    "- `Amazon Titan Text Embeddings V2`\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "### 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.9\n",
      "Boto3 SDK version: 1.38.26\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Local imports\n",
    "import utility\n",
    "\n",
    "# Print SDK versions\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Boto3 SDK version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial setup for clients and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base ID: VVVSF4ZHVN\n"
     ]
    }
   ],
   "source": [
    "bedrock_kb_name=\"knowledge-base-quick-start-xpwy7\"\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "aws_region = boto_session.region_name\n",
    "\n",
    "def get_knowledge_base_id(kb_name, region=\"us-east-1\"):\n",
    "    \"\"\"获取指定名称的知识库 ID\"\"\"\n",
    "    bedrock = boto3.client('bedrock-agent', region_name=aws_region)\n",
    "    \n",
    "    # 获取知识库列表\n",
    "    response = bedrock.list_knowledge_bases()\n",
    "    \n",
    "    # 查找匹配名称的知识库\n",
    "    for kb in response['knowledgeBaseSummaries']:\n",
    "        if kb['name'] == kb_name:\n",
    "            return kb['knowledgeBaseId']\n",
    "    \n",
    "    # 处理分页结果\n",
    "    while 'nextToken' in response:\n",
    "        response = bedrock.list_knowledge_bases(nextToken=response['nextToken'])\n",
    "        for kb in response['knowledgeBaseSummaries']:\n",
    "            if kb['name'] == kb_name:\n",
    "                return kb['knowledgeBaseId']\n",
    "    \n",
    "    return None\n",
    "\n",
    "bedrock_kb_id = get_knowledge_base_id(bedrock_kb_name)\n",
    "print(f\"Knowledge Base ID: {bedrock_kb_id}\")\n",
    "\n",
    "# bedrock_kb_id = aws bedrock list-knowledge-bases --region us-east-1 --profile global_ruiliang | jq -r '.knowledgeBaseSummaries[] | select(.name==\"YOUR_KB_NAME\") | .knowledgeBaseId'\n",
    "\n",
    "# %store -r bedrock_kb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Region: us-east-1\n",
      "Bedrock Knowledge Base ID: VVVSF4ZHVN\n"
     ]
    }
   ],
   "source": [
    "# Create boto3 session and set AWS region\n",
    "boto_session = boto3.Session()\n",
    "aws_region = boto_session.region_name\n",
    "\n",
    "# Create boto3 clients for Bedrock\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent-runtime', config=bedrock_config)\n",
    "\n",
    "# Set the Bedrock model to use for text generation\n",
    "model_id = 'amazon.nova-micro-v1:0'\n",
    "model_arn = f'arn:aws:bedrock:{aws_region}::foundation-model/{model_id}'\n",
    "\n",
    "# Print configurations\n",
    "print(\"AWS Region:\", aws_region)\n",
    "print(\"Bedrock Knowledge Base ID:\", bedrock_kb_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the **Retrieve API** with Foundation Models from Amazon Bedrock\n",
    "\n",
    "We will begin by defining a `retrieve` function that calls the `Retrieve` API provided by Amazon Bedrock Knowledge Bases (BKB). This API transforms the user query into vector embeddings, searches the connected knowledge base, and returns the most relevant results. This approach gives you fine-grained control to build custom RAG workflows on top of the retrieved content.\n",
    "\n",
    "The response from the `Retrieve` API includes several useful components:\n",
    "\n",
    "- The **retrieved document chunks** containing relevant content from the knowledge base  \n",
    "- The **source location type** and **URI** for each retrieved document, enabling traceability  \n",
    "- The **relevance score** for each document chunk, indicating how well it matches the query  \n",
    "\n",
    "Additionally, the `Retrieve` API supports the `overrideSearchType` parameter within `retrievalConfiguration`, allowing you to control the search strategy used:\n",
    "\n",
    "| Search Type | Description |\n",
    "|-------------|-------------|\n",
    "| `HYBRID`    | Combines semantic search (vector similarity) with keyword search for improved accuracy, especially for structured content. |\n",
    "| `SEMANTIC`  | Purely embedding-based semantic search, ideal for unstructured or natural language content. |\n",
    "\n",
    "By default, Amazon Bedrock automatically selects the optimal search strategy for your query. However, if needed, you can explicitly specify `HYBRID` or `SEMANTIC` using `overrideSearchType` to tailor the search behavior to your use case.\n",
    "\n",
    "### 2.1 Exploring the **Retrieve API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the `retrieve` function\n",
    "def retrieve(user_query, kb_id, num_of_results=5):\n",
    "    return bedrock_agent_client.retrieve(\n",
    "        retrievalQuery= {\n",
    "            'text': user_query\n",
    "        },\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration= {\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': num_of_results,\n",
    "                'overrideSearchType': \"HYBRID\", # optional\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Results:\n",
      " [\n",
      "  {\n",
      "    \"content\": {\n",
      "      \"text\": \"Our teams have developed low-cost antennas (i.e. customer terminals) that will lower the barriers to access. We recently unveiled the new terminals that will communicate with the satellites passing overhead, and we expect to be able to produce our standard residential version for less than $400 each. They're small: 11 inches square, 1 inch thick, and weigh less than 5 pounds without their mounting bracket, but they deliver speeds up to 400 megabits per second. And they're powered by Amazon-designed baseband chips. We're preparing to launch two prototype satellites to test the entire end-to-end communications network this year, and plan to be in beta with commercial customers in 2024. The customer reaction to what we've shared thus far about Kuiper has been very positive, and we believe Kuiper represents a very large potential opportunity for Amazon. It also shares several similarities to AWS in that it's capital intensive at the start, but has a large prospective consumer, enterprise, and government customer base, significant revenue and operating profit potential, and relatively few companies with the technical and inventive aptitude, as well as the investment hypothesis to go after it. One final investment area that I'll mention, that's core to setting Amazon up to invent in every area of our business for many decades to come, and where we're investing heavily is **Large Language Models (\\\"LLMs\\\") and Generative AI.** Machine learning has been a technology with high promise for several decades, but it's only been the last five to ten years that it's started to be used more pervasively by companies. This shift was driven by several factors, including access to higher volumes of compute capacity at lower prices than was ever available. Amazon has been using machine learning extensively for 25 years, employing it in everything from personalized ecommerce recommendations, to fulfillment center pick paths, to drones for Prime Air, to Alexa, to the many machine learning services AWS offers (where AWS has the broadest machine learning functionality and customer base of any cloud provider). More recently, a newer form of machine learning, called Generative AI, has burst onto the scene and promises to significantly accelerate machine learning adoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billions of parameters, and growing), across expansive datasets, and has radically general and broad recall and learning capabilities.\",\n",
      "      \"type\": \"TEXT\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"s3Location\": {\n",
      "        \"uri\": \"s3://bedrock-knowledge-base-quick-start/source_kb/AMZN-2022-Shareholder-Letter.pdf\"\n",
      "      },\n",
      "      \"type\": \"S3\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"x-amz-bedrock-kb-source-uri\": \"s3://bedrock-knowledge-base-quick-start/source_kb/AMZN-2022-Shareholder-Letter.pdf\",\n",
      "      \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AFCHaKpcBq0E1RWw79243\",\n",
      "      \"x-amz-bedrock-kb-data-source-id\": \"PVGDA2LV1I\"\n",
      "    },\n",
      "    \"score\": 0.6327392\n",
      "  },\n",
      "  {\n",
      "    \"content\": {\n",
      "      \"text\": \"More recently, a newer form of machine learning, called Generative AI, has burst onto the scene and promises to significantly accelerate machine learning adoption. Generative AI is based on very Large Language Models (trained on up to hundreds of billions of parameters, and growing), across expansive datasets, and has radically general and broad recall and learning capabilities. We have been working on our own LLMs for a while now, believe it will transform and improve virtually every customer experience, and will continue to invest substantially in these models across all of our consumer, seller, brand, and creator experiences. Additionally, as we've done for years in AWS, we're democratizing this technology so companies of all sizes can leverage Generative AI. AWS is offering the most price-performant machine learning chips in Trainium and Inferentia so small and large companies can afford to train and run their LLMs in production. We enable companies to choose from various LLMs and build applications with all of the AWS security, privacy and other features that customers are accustomed to using. And, we're delivering applications like AWS's Whisperer, which revolutionizes\",\n",
      "      \"type\": \"TEXT\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"s3Location\": {\n",
      "        \"uri\": \"s3://bedrock-knowledge-base-quick-start/source_kb/AMZN-2022-Shareholder-Letter.pdf\"\n",
      "      },\n",
      "      \"type\": \"S3\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"x-amz-bedrock-kb-source-uri\": \"s3://bedrock-knowledge-base-quick-start/source_kb/AMZN-2022-Shareholder-Letter.pdf\",\n",
      "      \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AFSHaKpcBq0E1RWw79243\",\n",
      "      \"x-amz-bedrock-kb-data-source-id\": \"PVGDA2LV1I\"\n",
      "    },\n",
      "    \"score\": 0.6327392\n",
      "  },\n",
      "  {\n",
      "    \"content\": {\n",
      "      \"text\": \"developer productivity by generating code suggestions in real time. I could write an entire letter on LLMs and Generative AI as I think they will be that transformative, but I'll leave that for a future letter. Let's just say that LLMs and Generative AI are going to be a big deal for customers, our shareholders, and Amazon. So, in closing, I'm optimistic that we'll emerge from this challenging macroeconomic time in a stronger position than when we entered it. There are several reasons for it and I've mentioned many of them above. But, there are two relatively simple statistics that underline our immense future opportunity. While we have a consumer business that's $434B in 2022, the vast majority of total market segment share in global retail still resides in physical stores (roughly 80%). And, it's a similar story for Global IT spending, where we have AWS revenue of $80B in 2022, with about 90% of Global IT spending still on-premises and yet to migrate to the cloud. As these equations steadily flip-as we're already seeing happen-we believe our leading customer experiences, relentless invention, customer focus, and hard work will result in significant growth in the coming years. And, of course, this doesn't include the other businesses and experiences we're pursuing at Amazon, all of which are still in their early days. I strongly believe that our best days are in front of us, and I look forward to working with my teammates at Amazon to make it so. Sincerely, ambuel R Jass, [SIGNATURE] Andy Jassy President and Chief Executive Officer Amazon.com, Inc. P.S. As we have always done, our original 1997 Shareholder Letter follows. What's written there is as true today as it was in 1997.\",\n",
      "      \"type\": \"TEXT\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"s3Location\": {\n",
      "        \"uri\": \"s3://bedrock-knowledge-base-quick-start/source_kb/AMZN-2022-Shareholder-Letter.pdf\"\n",
      "      },\n",
      "      \"type\": \"S3\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"x-amz-bedrock-kb-source-uri\": \"s3://bedrock-knowledge-base-quick-start/source_kb/AMZN-2022-Shareholder-Letter.pdf\",\n",
      "      \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AFiHaKpcBq0E1RWw79243\",\n",
      "      \"x-amz-bedrock-kb-data-source-id\": \"PVGDA2LV1I\"\n",
      "    },\n",
      "    \"score\": 0.5498960084053318\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What is Amazon doing in the field of Generative AI?\"\n",
    "\n",
    "response = retrieve(user_query, bedrock_kb_id, num_of_results=3)\n",
    "\n",
    "print(\"Retrieval Results:\\n\", json.dumps(response['retrievalResults'], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating a Response using Retrieved Context and the **Converse API**\n",
    "\n",
    "Once we have used the `Retrieve` API to fetch the most relevant document chunks from our knowledge base, the next step is to use this retrieved context to generate a grounded and informative response to the user query.\n",
    "\n",
    "In this section, we will construct a LLM request that combines both user query and the retrieved knowledge base content. We will then use Amazon Bedrock's `Converse` API to interact with a LLM of our choice to generate the final response.\n",
    "\n",
    "Specifically:\n",
    "- We will define a *system prompt* that provides general behavioral guidelines to the model — for example, instructing it to act like a financial advisor that prioritizes fact-based, concise answers.\n",
    "- We will create a *user prompt template* that injects both the retrieved context and the user’s query.\n",
    "- Finally, we will use the `Converse` API to generate the model’s response, ensuring that it leverages the provided context to produce accurate and grounded answers.\n",
    "\n",
    "This pattern enables full control over how context is presented to the model, allowing you to implement custom RAG workflows tailored to your application's needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a system prompt\n",
    "system_prompt = \"\"\"You are a financial advisor AI system, and provides answers to questions\n",
    "by using fact based and statistical information when possible. \n",
    "Use the following pieces of information in <context> tags to provide a concise answer to the questions.\n",
    "Give an answer directly, without any XML tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\"\"\n",
    "\n",
    "# Define a user prompt template\n",
    "user_prompt_template = \"\"\"Here is some additional context:\n",
    "<context>\n",
    "{contexts}\n",
    "</context>\n",
    "\n",
    "Please provide an answer to this user query:\n",
    "<query>\n",
    "{user_query}\n",
    "</query>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\"\"\"\n",
    "\n",
    "# Extract all context from all relevant retrieved document chunks\n",
    "contexts = [rr['content']['text'] for rr in response['retrievalResults']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer:\n",
      " Amazon is heavily investing in Large Language Models (LLMs) and Generative AI. They have developed their own LLMs and are working to integrate these models into various customer experiences across Amazon's consumer, seller, brand, and creator platforms. Amazon is also democratizing access to Generative AI through AWS, offering price-performant machine learning chips like Trainium and Inferentia, and providing a range of LLMs for companies of all sizes to leverage. AWS has also introduced applications like Whisperer, which enhances developer productivity by generating code suggestions in real time. Amazon believes that Generative AI will significantly accelerate machine learning adoption and transform customer experiences.\n"
     ]
    }
   ],
   "source": [
    "# Build Converse API request\n",
    "converse_request = {\n",
    "    \"system\": [\n",
    "        {\"text\": system_prompt}\n",
    "    ],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": user_prompt_template.format(contexts=contexts, user_query=user_query)\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"temperature\": 0.4,\n",
    "        \"topP\": 0.9,\n",
    "        \"maxTokens\": 500\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call Bedrock's Converse API to generate the final answer to user query\n",
    "response = bedrock_client.converse(\n",
    "    modelId=model_id,\n",
    "    system=converse_request['system'],\n",
    "    messages=converse_request[\"messages\"],\n",
    "    inferenceConfig=converse_request[\"inferenceConfig\"]\n",
    ")\n",
    "\n",
    "print(\"Final Answer:\\n\", response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusions and Next Steps\n",
    "\n",
    "In this notebook, we built a custom RAG-powered Q&A application using Amazon Bedrock Knowledge Bases and the `Retrieve` API.\n",
    "\n",
    "We followed three main steps:\n",
    "- Used the `Retrieve` API to fetch the most relevant document chunks from a knowledge base based on a user query.\n",
    "- Constructed an augmented prompt by combining the retrieved content with the user’s question.\n",
    "- Used the `Converse` API to generate a grounded, fact-based response leveraging the retrieved context.\n",
    "\n",
    "This approach provides flexibility and control over both search and response generation, enabling tailored RAG solutions for your specific use case.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Do not forget to clean up the resources here, if you do not indent to expriment with the created Bedrock Knowledge Base anymore:\n",
    "\n",
    "&nbsp; **NEXT ▶** [4_clean-up.ipynb](./4\\_clean-up.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
